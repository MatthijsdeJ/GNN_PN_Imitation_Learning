{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import auxiliary.util as util\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Union, List, Tuple, Optional\n",
    "\n",
    "from torch_geometric.data import Dataset\n",
    "#import imitation_data_loading\n",
    "#from tutor_data_loading import preprocess_observation, action_identificator, Episode, EpisodeStep, EpisodeSet, StepSet\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import wandb\n",
    "import training.training as trn\n",
    "from training.models import GCN, FCNN\n",
    "from training.dataloader import TutorDataLoader\n",
    "import collections\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.set_wd_to_package_root()\n",
    "config = util.load_config()\n",
    "processed_data_path = config['paths']['processed_tutor_imitation']\n",
    "matrix_cache_path = config['paths']['con_matrix_cache']\n",
    "feature_statistics_path = config['paths']['feature_statistics']\n",
    "action_counter_path = config['paths']['action_counter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo seperate static (line capacities, 'standard' voltage) and dynamic attributes\n",
    "#compare weights or/ex layers (sign?)\n",
    "#feature: number of connected nodes?\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = util.load_config()['training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "estimated_train_size = int(0.8*25993)\n",
    "label_smth_alpha = training_config['label_smoothing_alpha']\n",
    "\n",
    "\n",
    "with tqdm(total=n_epoch*estimated_train_size) as pbar:\n",
    "    model.zero_grad()\n",
    "    step=0\n",
    "    for e in range(n_epoch):\n",
    "        for datapoint in train_dl:\n",
    "            \n",
    "            #Get information from datapoint\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_W_sb_neigh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lin_gen_1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(abs(model.GNN_layers[6].convs['object__other_busbar__object'].lin_l.weight).sum())\n",
    "print(abs(model.GNN_layers[6].convs['object__same_busbar__object'].lin_r.weight).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.GNN_layers[6].convs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diffs = []\n",
    "for l in model.GNN_layers:\n",
    "    print(l)\n",
    "    if training_config['network_type'] == 'heterogenous':\n",
    "        norm_W_self = abs(l.convs['object__same_busbar__object'].lin_r.weight).sum()\n",
    "        norm_W_neigh = abs(l.convs['object__line__object'].lin_l.weight).sum() + \\\n",
    "                        abs(l.convs['object__same_busbar__object'].lin_l.weight).sum() + \\\n",
    "                        abs(l.convs['object__other_busbar__object'].lin_l.weight).sum()\n",
    "        diffs.append(norm_W_self-norm_W_neigh)\n",
    "    else:\n",
    "        assert False, 'Not developped yet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.GNN_layers[6].convs['object__same_busbar__object'].forward(torch.rand((1,32),device=device),\n",
    "                                                                  torch.tensor([],dtype=torch.long,device=device).reshape(2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.GNN_layers[6].convs['object__same_busbar__object'].lin_l.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config['network_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance analysis with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                                   else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNAdapter(GCN):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def forward()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config['training']\n",
    "if config['training']['hyperparams']['model_type'] == 'GCN':\n",
    "    model = GCNAdapter(train_config['hyperparams']['LReLu_neg_slope'],\n",
    "                         train_config['hyperparams']['weight_init_std'],\n",
    "                         train_config['GCN']['constants']['N_f_gen'],\n",
    "                         train_config['GCN']['constants']['N_f_load'],\n",
    "                         train_config['GCN']['constants']['N_f_endpoint'],\n",
    "                         train_config['GCN']['hyperparams']['N_GCN_layers'],\n",
    "                         train_config['hyperparams']['N_node_hidden'],\n",
    "                         train_config['GCN']['hyperparams']['aggr'],\n",
    "                         train_config['GCN']['hyperparams']['network_type'])\n",
    "else: \n",
    "    model = FCNN(train_config['hyperparams']['LReLu_neg_slope'],\n",
    "                                  train_config['hyperparams']['weight_init_std'],\n",
    "                                  train_config['FCNN']['constants']['size_in'],\n",
    "                                  train_config['FCNN']['constants']['size_out'],\n",
    "                                  train_config['FCNN']['hyperparams']['N_layers'],\n",
    "                                  train_config['hyperparams']['N_node_hidden'])\n",
    "model.load_state_dict(torch.load('models/FCNN_test',map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['training']['hyperparams']['model_type'] == 'GCN':\n",
    "    pass\n",
    "else:\n",
    "    val_dl = TutorDataLoader(processed_data_path + '/val', \n",
    "                 matrix_cache_path, \n",
    "                 feature_statistics_path, \n",
    "                 action_counter_path,\n",
    "                 device, \n",
    "                 FCNN,\n",
    "                 None,\n",
    "                 False,\n",
    "                 0)\n",
    "    \n",
    "    dps = torch.Tensor(0,config['training']['FCNN']['constants']['size_in'])\n",
    "\n",
    "    val_dl_iter = val_dl.__iter__()\n",
    "    for i in range(100):\n",
    "        dp = next(val_dl_iter)\n",
    "        dps = torch.cat((dps,dp['features'].reshape(1,-1)),axis=0)\n",
    "\n",
    "e = shap.DeepExplainer(model, dps)\n",
    "shap_values = e.shap_values(dps)\n",
    "mean_shap_over_dps = np.array(shap_values).mean(axis=(1))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shap_over_dps\n",
    "N_gen = 5\n",
    "N_f_gen= 3 #Number of generator features\n",
    "N_load = 11\n",
    "N_f_load= 3 #Number of load features\n",
    "N_line = 20\n",
    "N_f_endpoint= 6 #Number of endpoint (origin/extremity) features\n",
    "       \n",
    "c = 0\n",
    "mean_shap_x_gen = mean_shap_over_dps[:,0:N_gen*N_f_gen]\n",
    "c+=N_gen*N_f_gen\n",
    "mean_shap_x_load = mean_shap_over_dps[:,c:c+N_load*N_f_load]\n",
    "c+=N_load*N_f_load\n",
    "mean_shap_x_or = mean_shap_over_dps[:,c:c+N_line*N_f_endpoint]\n",
    "c+=N_line*N_f_endpoint\n",
    "mean_shap_x_ex = mean_shap_over_dps[:,c:c+N_line*N_f_endpoint]\n",
    "c+=N_line*N_f_endpoint\n",
    "mean_shap_x_topo_vect = mean_shap_over_dps[:,c:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shap_x_gen = mean_shap_x_gen.transpose().reshape(N_gen,N_f_gen,-1)\n",
    "mean_shap_x_load = mean_shap_x_load.transpose().reshape(N_load,N_f_load,-1)\n",
    "mean_shap_x_or = mean_shap_x_or.transpose().reshape(N_line,N_f_endpoint,-1)\n",
    "mean_shap_x_ex = mean_shap_x_ex.transpose().reshape(N_line,N_f_endpoint,-1)\n",
    "mean_shap_x_topo_vect = mean_shap_x_topo_vect.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(e.expected_value)),e.expected_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "axs[0,0].bar(['p','q','v'],mean_shap_x_gen.mean(axis=(0,2)))\n",
    "axs[0,1].bar(['p','q','v'],mean_shap_x_load.mean(axis=(0,2)))\n",
    "axs[1,0].bar(['p', 'q', 'v', 'a', 'line_rho','line_capacity'],mean_shap_x_or.mean(axis=(0,2)))\n",
    "axs[1,1].bar(['p', 'q', 'v', 'a', 'line_rho','line_capacity'],mean_shap_x_ex.mean(axis=(0,2)))\n",
    "axs[2,0].bar(range(len(mean_shap_x_topo_vect)),mean_shap_x_topo_vect.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "cumulative_sub_info = np.cumsum([3, 6, 4, 6, 5, 6, 3, 2, 5, 3, 3, 3, 4, 3])\n",
    "grid = np.concatenate([mean_shap_x_gen.mean(axis=1),\n",
    "                       mean_shap_x_load.mean(axis=1),\n",
    "                       mean_shap_x_or.mean(axis=1),\n",
    "                       mean_shap_x_ex.mean(axis=1)])\n",
    "ax = sns.heatmap(grid, cmap=\"YlGnBu\")\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "ax.set(xlabel='Output object', ylabel='Input object', title='Mean shapley values')\n",
    "ax.vlines(cumulative_sub_info, *ax.get_xlim())\n",
    "ax.hlines(cumulative_sub_info, *ax.get_xlim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cumulative_sub_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = mean_shap_x_topo_vect\n",
    "ax = sns.heatmap(grid, cmap=\"YlGnBu\")\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "ax.set(xlabel='Output object', ylabel='Input topo vect index', title='Mean shapley values')\n",
    "ax.vlines(cumulative_sub_info, *ax.get_xlim())\n",
    "ax.hlines(cumulative_sub_info, *ax.get_xlim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_shap_x_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(dps,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                                   else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = config['training']\n",
    "if config['training']['hyperparams']['model_type'] == 'GCN':\n",
    "    model = GCN(train_config['hyperparams']['LReLu_neg_slope'],\n",
    "                 train_config['hyperparams']['weight_init_std'],\n",
    "                 train_config['GCN']['constants']['N_f_gen'],\n",
    "                 train_config['GCN']['constants']['N_f_load'],\n",
    "                 train_config['GCN']['constants']['N_f_endpoint'],\n",
    "                 train_config['GCN']['hyperparams']['N_GNN_layers'],\n",
    "                 train_config['hyperparams']['N_node_hidden'],\n",
    "                 train_config['GCN']['hyperparams']['aggr'],\n",
    "                 train_config['GCN']['hyperparams']['network_type'])\n",
    "else: \n",
    "    model = FCNN(train_config['hyperparams']['LReLu_neg_slope'],\n",
    "                                  train_config['hyperparams']['weight_init_std'],\n",
    "                                  train_config['FCNN']['constants']['size_in'],\n",
    "                                  train_config['FCNN']['constants']['size_out'],\n",
    "                                  train_config['FCNN']['hyperparams']['N_layers'],\n",
    "                                  train_config['hyperparams']['N_node_hidden'])\n",
    "model.load_state_dict(torch.load('models/FCNN_test',map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = TutorDataLoader(processed_data_path + '/train', \n",
    "                 matrix_cache_path, \n",
    "                 feature_statistics_path, \n",
    "                 action_counter_path,\n",
    "                 device, \n",
    "                 FCNN,\n",
    "                 None,\n",
    "                 False,\n",
    "                 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacobs_gen_mean = torch.zeros(3)\n",
    "jacobs_load_mean = torch.zeros(3)\n",
    "jacobs_or_mean = torch.zeros(6)\n",
    "jacobs_ex_mean = torch.zeros(6)\n",
    "jacobs_object_sum = torch.zeros((56,56))\n",
    "\n",
    "Y_sum = torch.zeros(56)\n",
    "for i in range(100):\n",
    "    dp = next(val_dl.__iter__())\n",
    "    print(dp)\n",
    "    \n",
    "    if type(model) == GCN:\n",
    "        # Extract features\n",
    "        X_gen = dp['gen_features']\n",
    "        X_load = dp['load_features']\n",
    "        X_or = dp['or_features']\n",
    "        X_ex = dp['ex_features']\n",
    "\n",
    "        # Extract the position topology vector, which relates the\n",
    "        # objects ordered by type to their position in the topology vector\n",
    "        object_ptv = dp['object_ptv']\n",
    "\n",
    "        # Extract the edges\n",
    "        E = dp['edges']\n",
    "\n",
    "        # Pass through the model\n",
    "        P = model(X_gen, X_load, X_or, X_ex, E, object_ptv).reshape((-1))\n",
    "    elif type(model) == FCNN:\n",
    "        # Pass through the model\n",
    "        P = model(dp['features']).reshape((-1))\n",
    "\n",
    "    #Extract the label, apply label smoothing\n",
    "    Y = dp['change_topo_vect']\n",
    "    label_smth_alpha = train_config['hyperparams'] \\\n",
    "                                        ['label_smoothing_alpha']\n",
    "    Y_smth =(1-label_smth_alpha)*dp['change_topo_vect'] + \\\n",
    "              label_smth_alpha*0.5*torch.ones_like(Y,device=device)\n",
    "    Y_sum += Y\n",
    "    \n",
    "    #Compute the weights for the loss\n",
    "    non_sub_label_weight = train_config['hyperparams'] \\\n",
    "                                        ['non_sub_label_weight']\n",
    "    Y_sub_mask, Y_sub_idx = trn.get_Y_subchanged(Y,dp['sub_info'])\n",
    "    weights = trn.label_weights(1-Y_sub_mask,non_sub_label_weight)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Compute the loss, update gradients\n",
    "    l = trn.BCELoss_labels_weighted(P,Y_smth,weights)\n",
    "    l.backward()\n",
    "    \n",
    "    #Obtain the jacobian\n",
    "    \n",
    "    jacob = torch.autograd.functional.jacobian(func:= lambda xg, xl, xo, xe: \\\n",
    "                                   model(xg,xl,xo,xe,E,object_ptv),\n",
    "                                   inputs=(X_gen, X_load, X_or, X_ex))\n",
    "    \n",
    "    #Aggregate the jacobian to determine the importance of each feature\n",
    "    jacobs_gen_mean += torch.mean(torch.abs(out[0]),axis=(0,1,2))\n",
    "    jacobs_load_mean += torch.mean(torch.abs(out[1]),axis=(0,1,2))\n",
    "    jacobs_or_mean += torch.mean(torch.abs(out[2]),axis=(0,1,2))\n",
    "    jacobs_ex_mean += torch.mean(torch.abs(out[3]),axis=(0,1,2))\n",
    "    #jacobs_or_sum = torch.zeros(6)\n",
    "    #jacobs_ex_sum = torch.zeros(6)\n",
    "    \n",
    "    #Aggregate the jacobian to determine the importance of each object's features to each objects output\n",
    "    jacob_objects = torch.cat([torch.sum(torch.abs(out[0]),axis=(1,3)),\n",
    "        torch.sum(torch.abs(out[1]),axis=(1,3)),\n",
    "        torch.sum(torch.abs(out[2]),axis=(1,3)),\n",
    "        torch.sum(torch.abs(out[3]),axis=(1,3))],axis=1)[:,object_ptv]\n",
    "    jacobs_object_sum += jacob_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "axs[0,0].bar(['p','q','v'],jacobs_gen_mean)\n",
    "axs[0,1].bar(['p','q','v'],jacobs_load_mean)\n",
    "axs[1,0].bar(['p', 'q', 'v', 'a', 'line_rho','line_capacity'],jacobs_or_mean)\n",
    "axs[1,1].bar(['p', 'q', 'v', 'a', 'line_rho','line_capacity'],jacobs_ex_mean)\n",
    "#axs[0,0].bar(['p','q','v'],jacobs_gen_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "# gs = gridspec.GridSpec(1,2, height_ratios=[1,1], width_ratios=[1])\n",
    "# print(gs)\n",
    "# f = plt.figure()\n",
    "# ax1 = plt.subplot(gs[0])\n",
    "# ax2 = plt.subplot(gs[1])\n",
    "\n",
    "fig, axs = plt.subplots(1,2, gridspec_kw={'width_ratios': [0.65, 1]})\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "#f.subplots_adjust(hspace=0)\n",
    "axs[0].imshow(torch.log(Y_sum.reshape(-1,1)+0.1))\n",
    "axs[1].imshow(jacobs_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(jacobs_sum.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing whether the ordering checks out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TutorDataset(processed_data_path, matrix_cache_path, feature_statistics_path)\n",
    "dp = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ptv = dp['gen_pos_topo_vect']\n",
    "load_ptv = dp['load_pos_topo_vect']\n",
    "or_ptv = dp['line_or_pos_topo_vect']\n",
    "ex_ptv = dp['line_ex_pos_topo_vect']\n",
    "object_indices = np.argsort(np.concatenate([gen_ptv,load_ptv,or_ptv,ex_ptv]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_ptv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_dummy = len(gen_ptv)*[0]\n",
    "gen_dummy[-1] = 1000\n",
    "load_dummy = len(load_ptv)*[1]\n",
    "or_dummy = len(or_ptv)*[2]\n",
    "ex_dummy = len(ex_ptv)*[3]\n",
    "dummies = np.array(gen_dummy + load_dummy + or_dummy + ex_dummy)\n",
    "dummies[object_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummies_tensor = torch.cat([torch.tensor(d) for d in [gen_dummy,load_dummy,or_dummy,ex_dummy]])\n",
    "dummies_tensor[object_indices]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
